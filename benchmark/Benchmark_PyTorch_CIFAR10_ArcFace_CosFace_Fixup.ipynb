{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Benchmark_PyTorch_CIFAR10_ArcFace_CosFace_Fixup.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YCfzOodtacIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/hongyi-zhang/Fixup"
      ]
    },
    {
      "metadata": {
        "id": "qls5VN1HbQ_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/"
      ]
    },
    {
      "metadata": {
        "id": "5j-fD9M_a7of",
        "colab_type": "code",
        "outputId": "21937b3a-7842-4a87-f817-f6d2afc5aa57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/__init__.py\"\"\"\n",
        "#from .fixup_resnet_cifar import *\n",
        "#from .resnet_cifar import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/__init__.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "zq8ZgATIbIih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/fixup_resnet_cifar.py\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "__all__ = ['FixupResNet', 'fixup_resnet8', 'fixup_resnet20', 'fixup_resnet32', 'fixup_resnet44', 'fixup_resnet56', 'fixup_resnet110', 'fixup_resnet1202']\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class FixupBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(FixupBasicBlock, self).__init__()\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.bias1a = nn.Parameter(torch.zeros(1))\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bias1b = nn.Parameter(torch.zeros(1))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bias2a = nn.Parameter(torch.zeros(1))\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "        self.bias2b = nn.Parameter(torch.zeros(1))\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x + self.bias1a)\n",
        "        out = self.relu(out + self.bias1b)\n",
        "\n",
        "        out = self.conv2(out + self.bias2a)\n",
        "        out = out * self.scale + self.bias2b\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x + self.bias1a)\n",
        "            identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FixupResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(FixupResNet, self).__init__()\n",
        "        self.num_layers = sum(layers)\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(3, 16)\n",
        "        self.bias1 = nn.Parameter(torch.zeros(1))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.bias2 = nn.Parameter(torch.zeros(1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, FixupBasicBlock):\n",
        "                nn.init.normal_(m.conv1.weight, mean=0, std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))\n",
        "                nn.init.constant_(m.conv2.weight, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.AvgPool2d(1, stride=stride)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x + self.bias1)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x + self.bias2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def fixup_resnet8(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-8 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [1, 1, 1], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet20(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-20 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [3, 3, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet32(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-32 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [5, 5, 5], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet44(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-44 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [7, 7, 7], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet56(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-56 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [9, 9, 9], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet110(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-110 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [18, 18, 18], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fixup_resnet1202(**kwargs):\n",
        "    \"\"\"Constructs a Fixup-ResNet-1202 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = FixupResNet(FixupBasicBlock, [200, 200, 200], **kwargs)\n",
        "    return model    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqFy0kgSbnq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "jb6AJlqgbYr1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/resnet_cifar.py\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet8', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "            identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.num_layers = sum(layers)\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = conv3x3(3, 16)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.AvgPool2d(1, stride=stride),\n",
        "                nn.BatchNorm2d(self.inplanes),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet8(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-8 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [1, 1, 1], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet20(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-20 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 3, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet32(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-32 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [5, 5, 5], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet44(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-44 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [7, 7, 7], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet56(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-56 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [9, 9, 9], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet110(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-110 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [18, 18, 18], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet1202(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-1202 model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [200, 200, 200], **kwargs)\n",
        "    return model    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H2-qW_dCK1gP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Base code source: https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py \n",
        "Referece: \n",
        "ArcFace paper at https://arxiv.org/pdf/1801.07698.pdf\n",
        "CosFace paper at https://arxiv.org/pdf/1801.09414.pdf\n",
        "SphereFace paper at https://arxiv.org/pdf/1704.08063.pdf\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import math\n",
        "\n",
        "class LinearFWNorm(nn.Linear):\n",
        "\n",
        "    def forward(self,input):\n",
        "        #return F.linear(input, self.weight, self.bias)\n",
        "        return F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "\n",
        "def logit_where_and_scale(label, cosine_margined, cosine, s):\n",
        "    \"\"\" Use cosine_margined for the corresponding label, otherwise cosine.\n",
        "    \"\"\"   \n",
        "    one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "    one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "    #if torch.__version__ >= '0.4':\n",
        "    if True:\n",
        "        logit_output = torch.where(one_hot.byte(), cosine_margined, cosine) \n",
        "    #else:\n",
        "    #    logit_output = (one_hot * cosine_margined) + ((1.0 - one_hot) * cosine)  \n",
        "    logit_output *= s   \n",
        "    return logit_output\n",
        "\n",
        "class ArcFaceLoss(nn.CrossEntropyLoss):\n",
        "    r\"\"\" Receiving the logit as cos(theta), calculate s * cos(theta + m_arc) as the penalized logit, and calculate CrossEntropyLoss.\n",
        "        Args:\n",
        "            s: feature scale (64 in the ArcFace paper)\n",
        "            m_arc: additive angular margin m2; margin of angle in radian (0.5 in the ArcFace paper)           \n",
        "        \"\"\"\n",
        "    def __init__(self, s=64.0, m_arc=0.50, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.s = s\n",
        "        self.m_arc = m_arc\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m_arc)\n",
        "        self.sin_m = math.sin(m_arc)\n",
        "        self.th = math.cos(math.pi - m_arc)\n",
        "        self.mm = math.sin(math.pi - m_arc) * m_arc\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        cosine = input # output of LinearFWNorm\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)) # Trigonometric formula sin^2 + cos^2 = 1\n",
        "        cosine_margined = cosine * self.cos_m - sine * self.sin_m # Trigonometric Addition formula cos(a+b) = cos(a) cos(b) = sin(a) sin(b)\n",
        "        if self.easy_margin:\n",
        "            cosine_margined = torch.where(cosine > 0, cosine_margined, cosine)\n",
        "        else:\n",
        "            cosine_margined = torch.where(cosine > self.th, cosine_margined, cosine - self.mm)\n",
        "        \n",
        "        logit_output = logit_where_and_scale(label, cosine_margined, cosine, self.s)        \n",
        "        output = super().forward(logit_output, label)\n",
        "        return output\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + ', s=' + str(self.s) \\\n",
        "               + ', m_arc=' + str(self.m_arc) + ')'\n",
        "\n",
        "class CosFaceLoss(nn.CrossEntropyLoss):\n",
        "    r\"\"\" Receiving the logit as cos(theta), calculate s * (cos(theta) - m_cos) as the penalized logit, and calculate CrossEntropyLoss.\n",
        "    Args:\n",
        "        s: feature scale (64 in the CosFace paper)\n",
        "        m_cos: additive cosine margin m3; margin of cos(theta) (0.35 performed best on LFW and YTF according to the CosFace paper)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, s=64.0, m_cos=0.35):\n",
        "        super().__init__()\n",
        "        self.s = s\n",
        "        self.m_cos = m_cos\n",
        "\n",
        "    def forward(self, input, label):\n",
        "\n",
        "        cosine = input # output of LinearFWNorm\n",
        "        cosine_margined = cosine - self.m_cos        \n",
        "        logit_output = logit_where_and_scale(label, cosine_margined, cosine, self.s)        \n",
        "        output = super().forward(logit_output, label)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + ', s=' + str(self.s) \\\n",
        "               + ', m_cos=' + str(self.m_cos) + ')'\n",
        "\n",
        "\n",
        "class SphereFaceLoss(nn.CrossEntropyLoss):\n",
        "    r\"\"\" Receiving the logit as cos(theta), calculate s * cos(m_sphere * theta) as the penalized logit, and calculate CrossEntropyLoss.\n",
        "    Args:\n",
        "        m_sphere: multiplicative angular margin m1; margin to scale cos(theta) (1.35 suggested in in the ArcFace paper)\n",
        "    \"\"\"\n",
        "    def __init__(self, m_sphere=1.35):\n",
        "        super().__init__()\n",
        "        self.m_sphere = m_sphere\n",
        "        self.base = 1000.0\n",
        "        self.gamma = 0.12\n",
        "        self.power = 1\n",
        "        self.LambdaMin = 5.0\n",
        "        self.iter = 0\n",
        "\n",
        "        # duplication formula\n",
        "        self.mlambda = [\n",
        "            lambda x: x ** 0,\n",
        "            lambda x: x ** 1,\n",
        "            lambda x: 2 * x ** 2 - 1,\n",
        "            lambda x: 4 * x ** 3 - 3 * x,\n",
        "            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\n",
        "            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\n",
        "        ]\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\n",
        "        self.iter += 1\n",
        "        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n",
        "\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = input # output of LinearFWNorm\n",
        "        cosine = cosine.clamp(-1, 1)\n",
        "        cos_m_theta = self.mlambda[self.m_sphere](cosine)\n",
        "        theta = cosine.data.acos()\n",
        "        k = (self.m_sphere * theta / 3.14159265).floor()\n",
        "        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n",
        "        NormOfFeature = torch.norm(input, 2, 1)\n",
        "\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        one_hot = torch.zeros(cosine.size())\n",
        "        one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n",
        "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
        "\n",
        "        # --------------------------- Calculate output ---------------------------\n",
        "        logit_output = (one_hot * (phi_theta - cosine) / (1 + self.lamb)) + cosine\n",
        "        logit_output *= NormOfFeature.view(-1, 1)\n",
        "\n",
        "        output = super().forward(logit_output, label)        \n",
        "        \n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + ', m_sphere=' + str(self.m_sphere) + ')'\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJHzyAOdbwJa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/hongyi-zhang/Fixup/tree/master/cifar"
      ]
    },
    {
      "metadata": {
        "id": "PrCI2J83bvgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"https://github.com/hongyi-zhang/Fixup/blob/master/cifar/utils.py\"\"\"\n",
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True, per_sample=False):\n",
        "\n",
        "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    if alpha > 0. and not per_sample:\n",
        "        lam = torch.zeros(y.size()).fill_(np.random.beta(alpha, alpha)).cuda()\n",
        "        mixed_x = lam.view(-1, 1, 1, 1) * x + (1 - lam.view(-1, 1, 1, 1)) * x[index,:]\n",
        "    elif alpha > 0.:\n",
        "        lam = torch.Tensor(np.random.beta(alpha, alpha, size=y.size())).cuda()\n",
        "        mixed_x = lam.view(-1, 1, 1, 1) * x + (1 - lam.view(-1, 1, 1, 1)) * x[index,:]\n",
        "    else:\n",
        "        lam = torch.ones(y.size()).cuda()\n",
        "        mixed_x = x\n",
        "\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_lam_idx(batch_size, alpha, use_cuda=True):\n",
        "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0.:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    return lam, index    \n",
        "\n",
        "def mixup_criterion(y_a, y_b, lam):\n",
        "    return lambda criterion, pred: criterion(pred, y_a, lam) + criterion(pred, y_b, 1 - lam)\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "#_, term_width = os.popen('stty size', 'r').read().split()\n",
        "#term_width = int(term_width)\n",
        "term_width = 100\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "    \n",
        "    p = ''\n",
        "    p += ' [' #sys.stdout.write(' [')    \n",
        "    for i in range(cur_len):\n",
        "        p += '=' #sys.stdout.write('=')        \n",
        "    p += '>' #sys.stdout.write('>')    \n",
        "    for i in range(rest_len):\n",
        "        p += '.' #sys.stdout.write('.')        \n",
        "    p += ']' #sys.stdout.write(']')   \n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    p += msg #sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        p += ' ' #sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        p += '\\b' #'sys.stdout.write('\\b')\n",
        "    p += ' %d/%d ' % (current+1, total) #sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        p += '\\r' #sys.stdout.write('\\r')\n",
        "    else:\n",
        "        p += '\\n' #sys.stdout.write('\\n')\n",
        "    print(p, '\\r', end='', flush=True) #sys.stdout.flush()\n",
        "    \n",
        "    \n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8ToAywWMiFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"https://github.com/hongyi-zhang/Fixup/blob/master/cifar/cifar_train.py\"\"\"\n",
        "'''Train CIFAR10 with PyTorch.'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "#import models\n",
        "\n",
        "#from utils import progress_bar, mixup_data, mixup_criterion\n",
        "\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "\n",
        "import easydict\n",
        "from datetime import datetime\n",
        "\n",
        "# Training\n",
        "def train(net, trainloader, epoch, optimizer, loss_func, use_cuda):\n",
        "    #print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        # generate mixed inputs, two one-hot label vectors and mixing coefficient\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, args.alpha, use_cuda)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        if args.loss == 'mixup': \n",
        "            mixup_loss_func = mixup_criterion(targets_a, targets_b, lam)\n",
        "            loss = mixup_loss_func(criterion, outputs)\n",
        "        else:\n",
        "            loss = loss_func(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a.data).float()).cpu().sum() + ((1 - lam) * predicted.eq(targets_b.data).float()).cpu().sum()\n",
        "        #acc = 100.*float(correct)/float(total)\n",
        "        acc = float(correct)/float(total)\n",
        "\n",
        "        #progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        #    % (train_loss/(batch_idx+1), acc, correct, total))\n",
        "\n",
        "    return (train_loss/batch_idx, acc)\n",
        "\n",
        "def test(net, testloader, epoch, loss_func, use_cuda):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = net(inputs)\n",
        "            if args.loss == 'mixup': \n",
        "                loss = nn.CrossEntropyLoss()(outputs, targets)\n",
        "            else:\n",
        "                loss = loss_func(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            #    % (test_loss/(batch_idx+1), 100.*float(correct)/float(total), correct, total))\n",
        "\n",
        "        # Save checkpoint.\n",
        "        #acc = 100.*float(correct)/float(total)\n",
        "        acc = float(correct)/float(total)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            checkpoint(net, acc, epoch)\n",
        "\n",
        "    return (test_loss/batch_idx, acc)\n",
        "\n",
        "def checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    #print('Saving..')\n",
        "    state = {\n",
        "        'net': net,\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + args.arch + '_' + args.sess + '_' + str(args.seed) + '.ckpt')\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, base_learning_rate):\n",
        "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
        "    lr = base_learning_rate\n",
        "    if epoch <= 9 and lr > 0.1:\n",
        "        # warm-up training for large minibatch\n",
        "        lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if param_group['initial_lr'] == base_learning_rate:\n",
        "            param_group['lr'] = lr\n",
        "        else:\n",
        "            if epoch <= 9:\n",
        "                param_group['lr'] = param_group['initial_lr'] * lr / base_learning_rate\n",
        "            elif epoch < 100:\n",
        "                param_group['lr'] = param_group['initial_lr']\n",
        "            elif epoch < 150:\n",
        "                param_group['lr'] = param_group['initial_lr'] / 10.\n",
        "            else:\n",
        "                param_group['lr'] = param_group['initial_lr'] / 100.\n",
        "    return lr\n",
        "\n",
        "\n",
        "\n",
        "def timestamp():\n",
        "    return datetime.now().strftime('%Y-%m-%dT%H%M%S')\n",
        "\n",
        "def run_trial(args, logname):\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    numpy.random.seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    global best_acc\n",
        "    best_acc = 0  # best test accuracy\n",
        "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "    batch_size = args.batchsize\n",
        "    base_learning_rate = args.base_lr * args.batchsize / 128.\n",
        "    if use_cuda:\n",
        "        # data parallel\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        batch_size *= n_gpu\n",
        "        base_learning_rate *= n_gpu\n",
        "\n",
        "    # Data\n",
        "    print('==> Preparing data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # Model\n",
        "    if args.resume:\n",
        "        #assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "        if not os.path.isdir('checkpoint'): os.mkdir('checkpoint')\n",
        "        checkpoint_file = './checkpoint/ckpt.t7.' + args.sess + '_' + str(args.seed)\n",
        "    if args.resume and os.path.exists(checkpoint_file):\n",
        "        # Load checkpoint.\n",
        "        print('==> Resuming from checkpoint..')\n",
        "        checkpoint = torch.load(checkpoint_file)\n",
        "        net = checkpoint['net']\n",
        "        best_acc = checkpoint['acc']\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        torch.set_rng_state(checkpoint['rng_state'])\n",
        "    else:\n",
        "        print(\"=> creating model '{}'\".format(args.arch))\n",
        "        #net = models.__dict__[args.arch]()\n",
        "        net = globals().get(args.arch)()\n",
        "        if loss in ['cosface', 'arcface']:\n",
        "            num_ftrs = net.fc.in_features\n",
        "            num_classes = 10\n",
        "            net.fc = LinearFWNorm(num_ftrs, num_classes)\n",
        "\n",
        "    if use_cuda:\n",
        "        net.cuda()\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        print('Using', torch.cuda.device_count(), 'GPUs.')\n",
        "        cudnn.benchmark = True\n",
        "        print('Using CUDA..')\n",
        "\n",
        "\n",
        "    if args.loss == 'cosface':\n",
        "        loss_func = CosFaceLoss(s=64.0, m_cos=0.35)\n",
        "    if args.loss == 'arcface':\n",
        "        loss_func = ArcFaceLoss(s=64.0, m_arc=0.5)\n",
        "    if args.loss == 'cel': \n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    if args.loss == 'mixup':\n",
        "        criterion = lambda pred, target, lam: (-F.log_softmax(pred, dim=1) * torch.zeros(pred.size()).cuda().scatter_(1, target.data.view(-1, 1), lam.view(-1, 1))).sum(dim=1).mean()\n",
        "    parameters_bias = [p[1] for p in net.named_parameters() if 'bias' in p[0]]\n",
        "    parameters_scale = [p[1] for p in net.named_parameters() if 'scale' in p[0]]\n",
        "    parameters_others = [p[1] for p in net.named_parameters() if not ('bias' in p[0] or 'scale' in p[0])]\n",
        "    optimizer = optim.SGD(\n",
        "            [{'params': parameters_bias, 'lr': args.base_lr/10.}, \n",
        "            {'params': parameters_scale, 'lr': args.base_lr/10.}, \n",
        "            {'params': parameters_others}], \n",
        "            lr=base_learning_rate, \n",
        "            momentum=0.9, \n",
        "            weight_decay=args.decay)\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.exists(logname):\n",
        "        with open(logname, 'w') as logfile:\n",
        "            logwriter = csv.writer(logfile, delimiter=',')\n",
        "            logwriter.writerow(['epoch', 'lr', 'train loss', 'train acc', 'test loss', 'test acc', 'arch', 'loss', 'timestamp'])\n",
        "\n",
        "    sgdr = CosineAnnealingLR(optimizer, args.n_epoch, eta_min=0, last_epoch=-1)\n",
        "\n",
        "    print('### Started to train the model. | arch: {} | loss: {}'.format(args.arch, args.loss))\n",
        "    for epoch in range(start_epoch, args.n_epoch):\n",
        "        lr = 0.\n",
        "        if args.sgdr:\n",
        "            sgdr.step()\n",
        "            for param_group in optimizer.param_groups:\n",
        "                lr = param_group['lr']\n",
        "                break\n",
        "        else:\n",
        "            lr = adjust_learning_rate(optimizer, epoch, base_learning_rate)    \n",
        "        train_loss, train_acc = train(net, trainloader, epoch, optimizer, loss_func, use_cuda)\n",
        "        test_loss, test_acc = test(net, testloader, epoch, loss_func, use_cuda)\n",
        "        with open(logname, 'a') as logfile:\n",
        "            logwriter = csv.writer(logfile, delimiter=',')\n",
        "            logwriter.writerow([epoch, lr, train_loss, train_acc, test_loss, test_acc, args.arch, args.loss, timestamp()])\n",
        "        pritn_str = '### [{}] Epoch: {:3d} | LR: {:3f} | train_loss: {:6f} | train_acc: {:6f} | test_loss: {:6f} | test_acc: {:6f}'.format(timestamp(), epoch, lr, train_loss, train_acc, test_loss, test_acc)\n",
        "        print(pritn_str)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2Lp67xZ9LWl",
        "colab_type": "code",
        "outputId": "8eed659c-21b5-49d0-85a6-973db51ceb7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2002
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "    and callable(models.__dict__[name]))\n",
        "\"\"\"\n",
        "model_names = ['fixup_resnet_cifar', 'resnet_cifar', 'resnet_arcface_cifar', 'resnet_cosface_cifar']\n",
        "\n",
        "\"\"\"\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('-a', '--arch', metavar='ARCH', default='fixup_resnet110', choices=model_names, help='model architecture: ' +\n",
        "                        ' | '.join(model_names) + ' (default: fixup_resnet110)')\n",
        "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "parser.add_argument('--sess', default='mixup_default', type=str, help='session id')\n",
        "parser.add_argument('--seed', default=0, type=int, help='rng seed')\n",
        "parser.add_argument('--alpha', default=1., type=float, help='interpolation strength (uniform=1., ERM=0.)')\n",
        "parser.add_argument('--sgdr', action='store_true', help='use SGD with cosine annealing learning rate and restarts')\n",
        "parser.add_argument('--decay', default=1e-4, type=float, help='weight decay (default=1e-4)')\n",
        "parser.add_argument('--batchsize', default=128, type=int, help='batch size per GPU (default=128)')\n",
        "parser.add_argument('--n_epoch', default=200, type=int, help='total number of epochs')\n",
        "parser.add_argument('--base_lr', default=0.1, type=float, help='base learning rate (default=0.1)')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\"\"\"\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"arch\": 'resnet8',\n",
        "        \"resume\": False,\n",
        "        \"sess\": 'benchmark',\n",
        "        \"seed\": 0,\n",
        "        \"alpha\": 1.,\n",
        "        \"sgdr\": True,\n",
        "        \"decay\": 1e-4,\n",
        "        \"batchsize\": 128,\n",
        "        \"n_epoch\": 30,\n",
        "        \"base_lr\": 0.1,\n",
        "        #\"loss\": 'mixup'\n",
        "        \"loss\": 'cel'\n",
        "        #\"loss\": 'cosface'\n",
        "        #\"loss\": 'arcface'\n",
        "})\n",
        "\n",
        "result_folder = './results/'\n",
        "if not os.path.exists(result_folder):\n",
        "    os.makedirs(result_folder)\n",
        "        \n",
        "logname = result_folder + args.sess \n",
        "logname += '_seed_' + str(args.seed) + '_' + timestamp() + '.csv'\n",
        "\n",
        "loss_list = ['cel', 'cosface', 'arcface']\n",
        "# loss_list = ['cel']\n",
        "# arch_list = ['resnet8', 'fixup_resnet8']\n",
        "# arch_list = ['resnet20', 'fixup_resnet20']\n",
        "arch_list = ['resnet20']\n",
        "\n",
        "for loss in loss_list:\n",
        "    args.loss = loss\n",
        "    for arch in arch_list:\n",
        "        args.arch = arch\n",
        "        run_trial(args, logname)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "=> creating model 'resnet20'\n",
            "Using 1 GPUs.\n",
            "Using CUDA..\n",
            "### Started to train the model. | arch: resnet20 | loss: cel\n",
            "### [2019-04-16T144005] Epoch:   0 | LR: 0.010000 | train_loss: 2.210814 | train_acc: 0.210656 | test_loss: 1.953614 | test_acc: 0.303000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### [2019-04-16T144021] Epoch:   1 | LR: 0.009973 | train_loss: 2.155687 | train_acc: 0.270756 | test_loss: 1.832633 | test_acc: 0.379900\n",
            "### [2019-04-16T144037] Epoch:   2 | LR: 0.009891 | train_loss: 2.110551 | train_acc: 0.314149 | test_loss: 1.816447 | test_acc: 0.393100\n",
            "### [2019-04-16T144055] Epoch:   3 | LR: 0.009755 | train_loss: 2.078915 | train_acc: 0.346546 | test_loss: 1.674665 | test_acc: 0.444100\n",
            "### [2019-04-16T144111] Epoch:   4 | LR: 0.009568 | train_loss: 2.049814 | train_acc: 0.377299 | test_loss: 1.516799 | test_acc: 0.499300\n",
            "### [2019-04-16T144127] Epoch:   5 | LR: 0.009330 | train_loss: 2.032856 | train_acc: 0.406963 | test_loss: 1.552267 | test_acc: 0.503200\n",
            "### [2019-04-16T144143] Epoch:   6 | LR: 0.009045 | train_loss: 2.001226 | train_acc: 0.418550 | test_loss: 1.547571 | test_acc: 0.519300\n",
            "### [2019-04-16T144200] Epoch:   7 | LR: 0.008716 | train_loss: 1.962294 | train_acc: 0.430170 | test_loss: 1.372405 | test_acc: 0.568200\n",
            "### [2019-04-16T144217] Epoch:   8 | LR: 0.008346 | train_loss: 2.000321 | train_acc: 0.444256 | test_loss: 1.407964 | test_acc: 0.558100\n",
            "### [2019-04-16T144233] Epoch:   9 | LR: 0.007939 | train_loss: 1.982386 | train_acc: 0.441308 | test_loss: 1.358878 | test_acc: 0.607600\n",
            "### [2019-04-16T144249] Epoch:  10 | LR: 0.007500 | train_loss: 1.994866 | train_acc: 0.452072 | test_loss: 1.167038 | test_acc: 0.608500\n",
            "### [2019-04-16T144305] Epoch:  11 | LR: 0.007034 | train_loss: 1.943680 | train_acc: 0.467987 | test_loss: 1.310381 | test_acc: 0.602700\n",
            "### [2019-04-16T144322] Epoch:  12 | LR: 0.006545 | train_loss: 1.977354 | train_acc: 0.477149 | test_loss: 1.315167 | test_acc: 0.609800\n",
            "### [2019-04-16T144339] Epoch:  13 | LR: 0.006040 | train_loss: 1.919642 | train_acc: 0.476439 | test_loss: 1.272985 | test_acc: 0.616600\n",
            "### [2019-04-16T144355] Epoch:  14 | LR: 0.005523 | train_loss: 1.913322 | train_acc: 0.478897 | test_loss: 1.320131 | test_acc: 0.595200\n",
            "### [2019-04-16T144411] Epoch:  15 | LR: 0.005000 | train_loss: 1.882588 | train_acc: 0.498419 | test_loss: 1.271431 | test_acc: 0.646200\n",
            "### [2019-04-16T144427] Epoch:  16 | LR: 0.004477 | train_loss: 1.862376 | train_acc: 0.503873 | test_loss: 1.122576 | test_acc: 0.652900\n",
            "### [2019-04-16T144443] Epoch:  17 | LR: 0.003960 | train_loss: 1.906749 | train_acc: 0.516307 | test_loss: 1.365674 | test_acc: 0.630900\n",
            "### [2019-04-16T144500] Epoch:  18 | LR: 0.003455 | train_loss: 1.895729 | train_acc: 0.500053 | test_loss: 1.191400 | test_acc: 0.672000\n",
            "### [2019-04-16T144516] Epoch:  19 | LR: 0.002966 | train_loss: 1.893567 | train_acc: 0.518838 | test_loss: 1.200919 | test_acc: 0.658300\n",
            "### [2019-04-16T144531] Epoch:  20 | LR: 0.002500 | train_loss: 1.836365 | train_acc: 0.514728 | test_loss: 1.253739 | test_acc: 0.659600\n",
            "### [2019-04-16T144547] Epoch:  21 | LR: 0.002061 | train_loss: 1.805976 | train_acc: 0.520136 | test_loss: 1.031018 | test_acc: 0.682000\n",
            "### [2019-04-16T144603] Epoch:  22 | LR: 0.001654 | train_loss: 1.897608 | train_acc: 0.533820 | test_loss: 1.163021 | test_acc: 0.709100\n",
            "### [2019-04-16T144620] Epoch:  23 | LR: 0.001284 | train_loss: 1.866344 | train_acc: 0.521498 | test_loss: 1.069112 | test_acc: 0.707400\n",
            "### [2019-04-16T144635] Epoch:  24 | LR: 0.000955 | train_loss: 1.852151 | train_acc: 0.536621 | test_loss: 1.154053 | test_acc: 0.707600\n",
            "### [2019-04-16T144651] Epoch:  25 | LR: 0.000670 | train_loss: 1.784076 | train_acc: 0.532571 | test_loss: 0.964470 | test_acc: 0.716600\n",
            "### [2019-04-16T144706] Epoch:  26 | LR: 0.000432 | train_loss: 1.796081 | train_acc: 0.536062 | test_loss: 1.064140 | test_acc: 0.710200\n",
            "### [2019-04-16T144722] Epoch:  27 | LR: 0.000245 | train_loss: 1.861028 | train_acc: 0.553168 | test_loss: 1.079284 | test_acc: 0.718500\n",
            "### [2019-04-16T144739] Epoch:  28 | LR: 0.000109 | train_loss: 1.784703 | train_acc: 0.548033 | test_loss: 1.027490 | test_acc: 0.722400\n",
            "### [2019-04-16T144754] Epoch:  29 | LR: 0.000027 | train_loss: 1.741616 | train_acc: 0.548126 | test_loss: 1.016529 | test_acc: 0.724000\n",
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "=> creating model 'resnet20'\n",
            "Using 1 GPUs.\n",
            "Using CUDA..\n",
            "### Started to train the model. | arch: resnet20 | loss: cosface\n",
            "### [2019-04-16T144812] Epoch:   0 | LR: 0.010000 | train_loss: 25.386714 | train_acc: 0.125006 | test_loss: 24.866243 | test_acc: 0.156900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LinearFWNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### [2019-04-16T144828] Epoch:   1 | LR: 0.009973 | train_loss: 24.646872 | train_acc: 0.169426 | test_loss: 24.744155 | test_acc: 0.191100\n",
            "### [2019-04-16T144845] Epoch:   2 | LR: 0.009891 | train_loss: 24.601844 | train_acc: 0.199634 | test_loss: 24.557546 | test_acc: 0.270700\n",
            "### [2019-04-16T144903] Epoch:   3 | LR: 0.009755 | train_loss: 24.559863 | train_acc: 0.222325 | test_loss: 24.393747 | test_acc: 0.295000\n",
            "### [2019-04-16T144919] Epoch:   4 | LR: 0.009568 | train_loss: 24.515500 | train_acc: 0.243387 | test_loss: 24.273707 | test_acc: 0.311300\n",
            "### [2019-04-16T144934] Epoch:   5 | LR: 0.009330 | train_loss: 24.499659 | train_acc: 0.257738 | test_loss: 24.238698 | test_acc: 0.340300\n",
            "### [2019-04-16T144951] Epoch:   6 | LR: 0.009045 | train_loss: 24.481608 | train_acc: 0.267063 | test_loss: 24.234110 | test_acc: 0.359900\n",
            "### [2019-04-16T145007] Epoch:   7 | LR: 0.008716 | train_loss: 24.426747 | train_acc: 0.275379 | test_loss: 24.048345 | test_acc: 0.410800\n",
            "### [2019-04-16T145024] Epoch:   8 | LR: 0.008346 | train_loss: 24.458601 | train_acc: 0.285006 | test_loss: 24.106010 | test_acc: 0.352700\n",
            "### [2019-04-16T145040] Epoch:   9 | LR: 0.007939 | train_loss: 24.439869 | train_acc: 0.287991 | test_loss: 24.047987 | test_acc: 0.372600\n",
            "### [2019-04-16T145055] Epoch:  10 | LR: 0.007500 | train_loss: 24.437230 | train_acc: 0.300814 | test_loss: 24.066245 | test_acc: 0.350000\n",
            "### [2019-04-16T145111] Epoch:  11 | LR: 0.007034 | train_loss: 24.391307 | train_acc: 0.315515 | test_loss: 23.913656 | test_acc: 0.444000\n",
            "### [2019-04-16T145126] Epoch:  12 | LR: 0.006545 | train_loss: 24.396063 | train_acc: 0.333390 | test_loss: 23.956655 | test_acc: 0.441800\n",
            "### [2019-04-16T145144] Epoch:  13 | LR: 0.006040 | train_loss: 24.330825 | train_acc: 0.345971 | test_loss: 23.833278 | test_acc: 0.415600\n",
            "### [2019-04-16T145159] Epoch:  14 | LR: 0.005523 | train_loss: 24.309797 | train_acc: 0.360833 | test_loss: 23.650304 | test_acc: 0.506000\n",
            "### [2019-04-16T145215] Epoch:  15 | LR: 0.005000 | train_loss: 24.236429 | train_acc: 0.382256 | test_loss: 23.577826 | test_acc: 0.516500\n",
            "### [2019-04-16T145231] Epoch:  16 | LR: 0.004477 | train_loss: 24.199930 | train_acc: 0.399768 | test_loss: 23.097564 | test_acc: 0.535700\n",
            "### [2019-04-16T145246] Epoch:  17 | LR: 0.003960 | train_loss: 24.240705 | train_acc: 0.417389 | test_loss: 23.774758 | test_acc: 0.493300\n",
            "### [2019-04-16T145303] Epoch:  18 | LR: 0.003455 | train_loss: 24.194522 | train_acc: 0.420966 | test_loss: 23.246181 | test_acc: 0.537600\n",
            "### [2019-04-16T145319] Epoch:  19 | LR: 0.002966 | train_loss: 24.190047 | train_acc: 0.440164 | test_loss: 22.964138 | test_acc: 0.585800\n",
            "### [2019-04-16T145335] Epoch:  20 | LR: 0.002500 | train_loss: 24.078634 | train_acc: 0.439204 | test_loss: 23.439220 | test_acc: 0.579500\n",
            "### [2019-04-16T145352] Epoch:  21 | LR: 0.002061 | train_loss: 23.983179 | train_acc: 0.441841 | test_loss: 21.982562 | test_acc: 0.572200\n",
            "### [2019-04-16T145407] Epoch:  22 | LR: 0.001654 | train_loss: 24.175858 | train_acc: 0.464860 | test_loss: 22.952146 | test_acc: 0.603700\n",
            "### [2019-04-16T145425] Epoch:  23 | LR: 0.001284 | train_loss: 24.100635 | train_acc: 0.459719 | test_loss: 22.284012 | test_acc: 0.614200\n",
            "### [2019-04-16T145440] Epoch:  24 | LR: 0.000955 | train_loss: 24.031783 | train_acc: 0.475380 | test_loss: 22.913499 | test_acc: 0.614600\n",
            "### [2019-04-16T145457] Epoch:  25 | LR: 0.000670 | train_loss: 23.894575 | train_acc: 0.469731 | test_loss: 21.263783 | test_acc: 0.587500\n",
            "### [2019-04-16T145512] Epoch:  26 | LR: 0.000432 | train_loss: 23.915335 | train_acc: 0.475857 | test_loss: 21.729657 | test_acc: 0.628600\n",
            "### [2019-04-16T145528] Epoch:  27 | LR: 0.000245 | train_loss: 24.060211 | train_acc: 0.496195 | test_loss: 21.928251 | test_acc: 0.633000\n",
            "### [2019-04-16T145545] Epoch:  28 | LR: 0.000109 | train_loss: 23.821789 | train_acc: 0.494053 | test_loss: 21.358048 | test_acc: 0.632100\n",
            "### [2019-04-16T145601] Epoch:  29 | LR: 0.000027 | train_loss: 23.670633 | train_acc: 0.492574 | test_loss: 21.245953 | test_acc: 0.627800\n",
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "=> creating model 'resnet20'\n",
            "Using 1 GPUs.\n",
            "Using CUDA..\n",
            "### Started to train the model. | arch: resnet20 | loss: arcface\n",
            "### [2019-04-16T145618] Epoch:   0 | LR: 0.010000 | train_loss: 26.653636 | train_acc: 0.116968 | test_loss: 17.716050 | test_acc: 0.122300\n",
            "### [2019-04-16T145634] Epoch:   1 | LR: 0.009973 | train_loss: 17.555469 | train_acc: 0.133922 | test_loss: 17.642072 | test_acc: 0.178100\n",
            "### [2019-04-16T145650] Epoch:   2 | LR: 0.009891 | train_loss: 17.496367 | train_acc: 0.168971 | test_loss: 17.498823 | test_acc: 0.197700\n",
            "### [2019-04-16T145707] Epoch:   3 | LR: 0.009755 | train_loss: 17.468676 | train_acc: 0.182078 | test_loss: 17.432992 | test_acc: 0.227300\n",
            "### [2019-04-16T145723] Epoch:   4 | LR: 0.009568 | train_loss: 17.433949 | train_acc: 0.206618 | test_loss: 17.276136 | test_acc: 0.258700\n",
            "### [2019-04-16T145739] Epoch:   5 | LR: 0.009330 | train_loss: 17.406952 | train_acc: 0.240950 | test_loss: 17.050064 | test_acc: 0.281500\n",
            "### [2019-04-16T145755] Epoch:   6 | LR: 0.009045 | train_loss: 17.375154 | train_acc: 0.254799 | test_loss: 17.236942 | test_acc: 0.301500\n",
            "### [2019-04-16T145811] Epoch:   7 | LR: 0.008716 | train_loss: 17.327962 | train_acc: 0.269020 | test_loss: 17.084823 | test_acc: 0.349700\n",
            "### [2019-04-16T145828] Epoch:   8 | LR: 0.008346 | train_loss: 17.355009 | train_acc: 0.284672 | test_loss: 16.865949 | test_acc: 0.361900\n",
            "### [2019-04-16T145843] Epoch:   9 | LR: 0.007939 | train_loss: 17.333854 | train_acc: 0.294147 | test_loss: 17.006921 | test_acc: 0.347900\n",
            "### [2019-04-16T145901] Epoch:  10 | LR: 0.007500 | train_loss: 17.326940 | train_acc: 0.311425 | test_loss: 16.768198 | test_acc: 0.373900\n",
            "### [2019-04-16T145917] Epoch:  11 | LR: 0.007034 | train_loss: 17.261319 | train_acc: 0.326415 | test_loss: 16.826155 | test_acc: 0.432200\n",
            "### [2019-04-16T145933] Epoch:  12 | LR: 0.006545 | train_loss: 17.286028 | train_acc: 0.345366 | test_loss: 16.927594 | test_acc: 0.401800\n",
            "### [2019-04-16T145950] Epoch:  13 | LR: 0.006040 | train_loss: 17.214708 | train_acc: 0.348593 | test_loss: 16.484126 | test_acc: 0.448900\n",
            "### [2019-04-16T150007] Epoch:  14 | LR: 0.005523 | train_loss: 17.209365 | train_acc: 0.356810 | test_loss: 16.683081 | test_acc: 0.477800\n",
            "### [2019-04-16T150023] Epoch:  15 | LR: 0.005000 | train_loss: 17.138554 | train_acc: 0.371631 | test_loss: 16.630991 | test_acc: 0.480500\n",
            "### [2019-04-16T150038] Epoch:  16 | LR: 0.004477 | train_loss: 17.117956 | train_acc: 0.381801 | test_loss: 16.048328 | test_acc: 0.495100\n",
            "### [2019-04-16T150055] Epoch:  17 | LR: 0.003960 | train_loss: 17.135904 | train_acc: 0.399607 | test_loss: 16.728087 | test_acc: 0.497500\n",
            "### [2019-04-16T150111] Epoch:  18 | LR: 0.003455 | train_loss: 17.132781 | train_acc: 0.399142 | test_loss: 16.315001 | test_acc: 0.505600\n",
            "### [2019-04-16T150127] Epoch:  19 | LR: 0.002966 | train_loss: 17.104140 | train_acc: 0.416854 | test_loss: 16.445332 | test_acc: 0.524700\n",
            "### [2019-04-16T150143] Epoch:  20 | LR: 0.002500 | train_loss: 16.994723 | train_acc: 0.420027 | test_loss: 16.575690 | test_acc: 0.534000\n",
            "### [2019-04-16T150159] Epoch:  21 | LR: 0.002061 | train_loss: 16.918268 | train_acc: 0.431098 | test_loss: 15.653274 | test_acc: 0.559400\n",
            "### [2019-04-16T150215] Epoch:  22 | LR: 0.001654 | train_loss: 17.065515 | train_acc: 0.447752 | test_loss: 16.460873 | test_acc: 0.533700\n",
            "### [2019-04-16T150232] Epoch:  23 | LR: 0.001284 | train_loss: 17.026666 | train_acc: 0.442657 | test_loss: 15.805154 | test_acc: 0.586500\n",
            "### [2019-04-16T150247] Epoch:  24 | LR: 0.000955 | train_loss: 16.944264 | train_acc: 0.459206 | test_loss: 16.297635 | test_acc: 0.593600\n",
            "### [2019-04-16T150303] Epoch:  25 | LR: 0.000670 | train_loss: 16.848427 | train_acc: 0.457155 | test_loss: 14.912381 | test_acc: 0.596600\n",
            "### [2019-04-16T150319] Epoch:  26 | LR: 0.000432 | train_loss: 16.839112 | train_acc: 0.461450 | test_loss: 15.807549 | test_acc: 0.600400\n",
            "### [2019-04-16T150336] Epoch:  27 | LR: 0.000245 | train_loss: 16.967702 | train_acc: 0.478939 | test_loss: 15.939717 | test_acc: 0.609100\n",
            "### [2019-04-16T150352] Epoch:  28 | LR: 0.000109 | train_loss: 16.823977 | train_acc: 0.476717 | test_loss: 15.432062 | test_acc: 0.613500\n",
            "### [2019-04-16T150409] Epoch:  29 | LR: 0.000027 | train_loss: 16.680865 | train_acc: 0.477703 | test_loss: 15.389093 | test_acc: 0.611100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T1GhpVl7eoix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}